{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Single-Core Scratch Implementation"
      ],
      "metadata": {
        "id": "mruwHd0fefBD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import csv\n",
        "import time\n",
        "import psutil\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "cifar10 = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "images = np.stack([np.array(img[0].numpy(), dtype=np.float32) for img in cifar10])  # shape: [50000, 3, 32, 32]\n",
        "\n",
        "# Define ReLU\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "# Define im2col\n",
        "def im2col(input_data, kernel_h, kernel_w, stride=1, padding=0):\n",
        "    N, C, H, W = input_data.shape\n",
        "    out_h = (H + 2*padding - kernel_h) // stride + 1\n",
        "    out_w = (W + 2*padding - kernel_w) // stride + 1\n",
        "    img = np.pad(input_data, ((0,0), (0,0), (padding, padding), (padding, padding)), mode='constant')\n",
        "    col = np.zeros((N, C, kernel_h, kernel_w, out_h, out_w))\n",
        "    for y in range(kernel_h):\n",
        "        y_max = y + stride*out_h\n",
        "        for x in range(kernel_w):\n",
        "            x_max = x + stride*out_w\n",
        "            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n",
        "    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N*out_h*out_w, -1)\n",
        "    return col\n",
        "\n",
        "# Convolution\n",
        "def conv2d_im2col(x, weight, bias, stride=1, padding=1):\n",
        "    N, C, H, W = x.shape\n",
        "    F, _, KH, KW = weight.shape\n",
        "    OH = (H + 2 * padding - KH) // stride + 1\n",
        "    OW = (W + 2 * padding - KW) // stride + 1\n",
        "    col = im2col(x, KH, KW, stride, padding)\n",
        "    weight_col = weight.reshape(F, -1)\n",
        "    out = col @ weight_col.T + bias\n",
        "    return out.reshape(N, OH, OW, F).transpose(0, 3, 1, 2)\n",
        "\n",
        "# ResNet Blocks\n",
        "class ResNet18CIFAR:\n",
        "    def __init__(self):\n",
        "        # Initial Conv\n",
        "        self.conv1_W = np.random.randn(64, 3, 3, 3).astype(np.float32) * 0.01\n",
        "        self.conv1_b = np.zeros(64, dtype=np.float32)\n",
        "\n",
        "        # Define weights\n",
        "        self.id_blocks = []\n",
        "        self.ds_blocks = []\n",
        "\n",
        "        # Identity blocks\n",
        "        for _ in range(2):\n",
        "            W1 = np.random.randn(64, 64, 3, 3).astype(np.float32) * 0.01\n",
        "            b1 = np.zeros(64, dtype=np.float32)\n",
        "            W2 = np.random.randn(64, 64, 3, 3).astype(np.float32) * 0.01\n",
        "            b2 = np.zeros(64, dtype=np.float32)\n",
        "            self.id_blocks.append((W1, b1, W2, b2))\n",
        "\n",
        "        # Downsample + identity: (64→128), (128→256), (256→512)\n",
        "        channels = [(64, 128), (128, 256), (256, 512)]\n",
        "        for in_c, out_c in channels:\n",
        "            W1 = np.random.randn(out_c, in_c, 3, 3).astype(np.float32) * 0.01\n",
        "            b1 = np.zeros(out_c, dtype=np.float32)\n",
        "            W2 = np.random.randn(out_c, out_c, 3, 3).astype(np.float32) * 0.01\n",
        "            b2 = np.zeros(out_c, dtype=np.float32)\n",
        "            W_short = np.random.randn(out_c, in_c, 1, 1).astype(np.float32) * 0.01\n",
        "            b_short = np.zeros(out_c, dtype=np.float32)\n",
        "            self.ds_blocks.append((W1, b1, W2, b2, W_short, b_short))\n",
        "\n",
        "        # Final FC\n",
        "        self.fc_W = np.random.randn(512, 10).astype(np.float32) * 0.01\n",
        "        self.fc_b = np.zeros(10, dtype=np.float32)\n",
        "\n",
        "    def identity_block(self, x, W1, b1, W2, b2):\n",
        "        x_shortcut = x.copy()\n",
        "        x = relu(conv2d_im2col(x, W1, b1, stride=1, padding=1))\n",
        "        x = conv2d_im2col(x, W2, b2, stride=1, padding=1)\n",
        "        return relu(x + x_shortcut)\n",
        "\n",
        "    def downsample_block(self, x, W1, b1, W2, b2, W_short, b_short, stride=2):\n",
        "        x_shortcut = conv2d_im2col(x, W_short, b_short, stride=stride, padding=0)\n",
        "        x = relu(conv2d_im2col(x, W1, b1, stride=stride, padding=1))\n",
        "        x = conv2d_im2col(x, W2, b2, stride=1, padding=1)\n",
        "        return relu(x + x_shortcut)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = relu(conv2d_im2col(x, self.conv1_W, self.conv1_b, stride=1, padding=1))\n",
        "        for W1, b1, W2, b2 in self.id_blocks:\n",
        "            x = self.identity_block(x, W1, b1, W2, b2)\n",
        "        for idx, (W1, b1, W2, b2, W_short, b_short) in enumerate(self.ds_blocks):\n",
        "            x = self.downsample_block(x, W1, b1, W2, b2, W_short, b_short, stride=2)\n",
        "\n",
        "            id_W1, id_b1 = np.random.randn(W2.shape[0], W2.shape[0], 3, 3).astype(np.float32) * 0.01, np.zeros(W2.shape[0])\n",
        "            id_W2, id_b2 = np.random.randn(W2.shape[0], W2.shape[0], 3, 3).astype(np.float32) * 0.01, np.zeros(W2.shape[0])\n",
        "\n",
        "            x = self.identity_block(x, id_W1, id_b1, id_W2, id_b2)\n",
        "        x = x.mean(axis=(2, 3))  # GAP\n",
        "        return x @ self.fc_W + self.fc_b\n",
        "\n",
        "model = ResNet18CIFAR()\n",
        "\n",
        "# Logging\n",
        "with open('resnet18_cpu_forwardpass_cifar10.csv', mode='w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"Image_Index\", \"CPU (%)\", \"RAM (MB)\", \"Time (s)\", \"Throughput (img/s)\"])\n",
        "    for i, image in enumerate(images):\n",
        "        if i % 1000 == 0:\n",
        "            print(f\"Processing image {i}...\")\n",
        "        x = image[None, :, :, :]\n",
        "\n",
        "        start = time.time()\n",
        "        _ = model.forward(x)\n",
        "        end = time.time()\n",
        "\n",
        "        elapsed = end - start\n",
        "        cpu = psutil.cpu_percent()\n",
        "        ram = psutil.virtual_memory().used / (1024 ** 2)\n",
        "        throughput = 1 / elapsed if elapsed > 0 else 0\n",
        "\n",
        "        writer.writerow([i, cpu, ram, elapsed, throughput])"
      ],
      "metadata": {
        "id": "Xxp9Qcr7eRzV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multi-Core Scratch Implementation"
      ],
      "metadata": {
        "id": "oKzzipyDexqv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import csv\n",
        "import time\n",
        "import psutil\n",
        "import multiprocessing\n",
        "from torchvision import datasets, transforms\n",
        "from concurrent.futures import ProcessPoolExecutor\n",
        "\n",
        "==\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "cifar10 = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "images = np.stack([np.array(img[0].numpy(), dtype=np.float32) for img in cifar10])  # [50000, 3, 32, 32]\n",
        "\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def im2col(input_data, kernel_h, kernel_w, stride=1, padding=0):\n",
        "    N, C, H, W = input_data.shape\n",
        "    out_h = (H + 2 * padding - kernel_h) // stride + 1\n",
        "    out_w = (W + 2 * padding - kernel_w) // stride + 1\n",
        "    img = np.pad(input_data, ((0, 0), (0, 0), (padding, padding), (padding, padding)), mode='constant')\n",
        "    col = np.zeros((N, C, kernel_h, kernel_w, out_h, out_w))\n",
        "    for y in range(kernel_h):\n",
        "        y_max = y + stride * out_h\n",
        "        for x in range(kernel_w):\n",
        "            x_max = x + stride * out_w\n",
        "            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n",
        "    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N * out_h * out_w, -1)\n",
        "    return col\n",
        "\n",
        "def conv2d_im2col(x, weight, bias, stride=1, padding=1):\n",
        "    N, C, H, W = x.shape\n",
        "    F, _, KH, KW = weight.shape\n",
        "    OH = (H + 2 * padding - KH) // stride + 1\n",
        "    OW = (W + 2 * padding - KW) // stride + 1\n",
        "    col = im2col(x, KH, KW, stride, padding)\n",
        "    weight_col = weight.reshape(F, -1)\n",
        "    out = col @ weight_col.T + bias\n",
        "    return out.reshape(N, OH, OW, F).transpose(0, 3, 1, 2)\n",
        "\n",
        "\n",
        "\n",
        "class ResNet18CIFAR:\n",
        "    def __init__(self):\n",
        "        self.conv1_W = np.random.randn(64, 3, 3, 3).astype(np.float32) * 0.01\n",
        "        self.conv1_b = np.zeros(64, dtype=np.float32)\n",
        "\n",
        "        self.id_blocks = []\n",
        "        self.ds_blocks = []\n",
        "\n",
        "        for _ in range(2):\n",
        "            W1 = np.random.randn(64, 64, 3, 3).astype(np.float32) * 0.01\n",
        "            b1 = np.zeros(64, dtype=np.float32)\n",
        "            W2 = np.random.randn(64, 64, 3, 3).astype(np.float32) * 0.01\n",
        "            b2 = np.zeros(64, dtype=np.float32)\n",
        "            self.id_blocks.append((W1, b1, W2, b2))\n",
        "\n",
        "        channels = [(64, 128), (128, 256), (256, 512)]\n",
        "        for in_c, out_c in channels:\n",
        "            W1 = np.random.randn(out_c, in_c, 3, 3).astype(np.float32) * 0.01\n",
        "            b1 = np.zeros(out_c, dtype=np.float32)\n",
        "            W2 = np.random.randn(out_c, out_c, 3, 3).astype(np.float32) * 0.01\n",
        "            b2 = np.zeros(out_c, dtype=np.float32)\n",
        "            W_short = np.random.randn(out_c, in_c, 1, 1).astype(np.float32) * 0.01\n",
        "            b_short = np.zeros(out_c, dtype=np.float32)\n",
        "            self.ds_blocks.append((W1, b1, W2, b2, W_short, b_short))\n",
        "\n",
        "        self.fc_W = np.random.randn(512, 10).astype(np.float32) * 0.01\n",
        "        self.fc_b = np.zeros(10, dtype=np.float32)\n",
        "\n",
        "    def identity_block(self, x, W1, b1, W2, b2):\n",
        "        x_shortcut = x.copy()\n",
        "        x = relu(conv2d_im2col(x, W1, b1, stride=1, padding=1))\n",
        "        x = conv2d_im2col(x, W2, b2, stride=1, padding=1)\n",
        "        return relu(x + x_shortcut)\n",
        "\n",
        "    def downsample_block(self, x, W1, b1, W2, b2, W_short, b_short, stride=2):\n",
        "        x_shortcut = conv2d_im2col(x, W_short, b_short, stride=stride, padding=0)\n",
        "        x = relu(conv2d_im2col(x, W1, b1, stride=stride, padding=1))\n",
        "        x = conv2d_im2col(x, W2, b2, stride=1, padding=1)\n",
        "        return relu(x + x_shortcut)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = relu(conv2d_im2col(x, self.conv1_W, self.conv1_b, stride=1, padding=1))\n",
        "        for W1, b1, W2, b2 in self.id_blocks:\n",
        "            x = self.identity_block(x, W1, b1, W2, b2)\n",
        "        for idx, (W1, b1, W2, b2, W_short, b_short) in enumerate(self.ds_blocks):\n",
        "            x = self.downsample_block(x, W1, b1, W2, b2, W_short, b_short, stride=2)\n",
        "            id_W1 = np.random.randn(W2.shape[0], W2.shape[0], 3, 3).astype(np.float32) * 0.01\n",
        "            id_b1 = np.zeros(W2.shape[0])\n",
        "            id_W2 = np.random.randn(W2.shape[0], W2.shape[0], 3, 3).astype(np.float32) * 0.01\n",
        "            id_b2 = np.zeros(W2.shape[0])\n",
        "            x = self.identity_block(x, id_W1, id_b1, id_W2, id_b2)\n",
        "        x = x.mean(axis=(2, 3))\n",
        "        return x @ self.fc_W + self.fc_b\n",
        "\n",
        "\n",
        "\n",
        "def process_image(i):\n",
        "    import time\n",
        "    import psutil\n",
        "\n",
        "    model = ResNet18CIFAR()\n",
        "    x = images[i][None, :, :, :]\n",
        "\n",
        "    start = time.time()\n",
        "    _ = model.forward(x)\n",
        "    end = time.time()\n",
        "\n",
        "    elapsed = end - start\n",
        "    cpu = psutil.cpu_percent(interval=None)\n",
        "    ram = psutil.virtual_memory().used / (1024 ** 2)\n",
        "    throughput = 1 / elapsed if elapsed > 0 else 0\n",
        "\n",
        "    return [i, cpu, ram, elapsed, throughput]\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    num_workers = multiprocessing.cpu_count()\n",
        "\n",
        "    with open('resnet18_multicore_scratch_window.csv', mode='w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow([\"Image_Index\", \"CPU (%)\", \"RAM (MB)\", \"Time (s)\", \"Throughput (img/s)\"])\n",
        "\n",
        "        with ProcessPoolExecutor(max_workers=num_workers) as executor:\n",
        "            for i, result in enumerate(executor.map(process_image, range(len(images)))):\n",
        "                if i % 1000 == 0:\n",
        "                    print(f\"Completed {i} images...\")\n",
        "                writer.writerow(result)"
      ],
      "metadata": {
        "id": "yxhlTvPme2QO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Single-Core PyTorch Implementation"
      ],
      "metadata": {
        "id": "Vfe0ESkZx-Na"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import time\n",
        "import psutil\n",
        "import csv\n",
        "import os\n",
        "from torchvision.models import resnet18\n",
        "\n",
        "# Use a single CPU core\n",
        "torch.set_num_threads(1)\n",
        "torch.set_num_interop_threads(1)\n",
        "print(f\"[CPU mode] PyTorch using {torch.get_num_threads()} thread\")\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "# Transform\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Datasets\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Model\n",
        "model = resnet18(num_classes=10)\n",
        "model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "model.maxpool = nn.Identity()\n",
        "model = model.to(device)\n",
        "\n",
        "# Loss and Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "# Accuracy Function\n",
        "def calculate_accuracy(loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loader:\n",
        "            outputs = model(images.to(device))\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct += (predicted == labels.to(device)).sum().item()\n",
        "            total += labels.size(0)\n",
        "    model.train()\n",
        "    return 100 * correct / total\n",
        "\n",
        "# CSV File\n",
        "csv_file = 'resnet18_single_core_log_pytorch.csv'\n",
        "\n",
        "# Training\n",
        "num_epochs = 10\n",
        "last_row = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    total_train = 0\n",
        "    correct_train = 0\n",
        "    epoch_loss = 0.0\n",
        "    batch_count = 0\n",
        "    start_epoch_time = time.perf_counter()\n",
        "\n",
        "    for batch_idx, (images, labels) in enumerate(trainloader):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += (predicted == labels).sum().item()\n",
        "        epoch_loss = loss.item()\n",
        "        batch_count += 1\n",
        "\n",
        "    end_epoch_time = time.perf_counter()\n",
        "    elapsed = end_epoch_time - start_epoch_time\n",
        "    train_acc = 100 * correct_train / total_train\n",
        "    test_acc = calculate_accuracy(testloader)\n",
        "    cpu = psutil.cpu_percent(interval=None)\n",
        "    ram = psutil.Process(os.getpid()).memory_info().rss / (1024 ** 2)\n",
        "\n",
        "\n",
        "    last_row = [\n",
        "        epoch + 1, batch_count, f\"{epoch_loss:.4f}\",\n",
        "        f\"{train_acc:.2f}\", f\"{test_acc:.2f}\",\n",
        "        f\"{cpu:.2f}\", f\"{ram:.2f}\", f\"{elapsed:.6f}\"\n",
        "    ]\n",
        "\n",
        "    print(f\"[Epoch {epoch+1}] Final: Loss={epoch_loss:.4f}, \"\n",
        "          f\"Train Acc={train_acc:.2f}%, Test Acc={test_acc:.2f}%, \"\n",
        "          f\"CPU={cpu:.2f}%, RAM={ram:.2f}MB, Time={elapsed:.6f}s\")\n",
        "\n",
        "\n",
        "with open(csv_file, mode='w', newline='') as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow([\"Epoch\", \"Batch_Index\", \"Loss\", \"Training_Accuracy(%)\", \"Testing_Accuracy(%)\",\n",
        "                     \"CPU_Usage(%)\", \"RAM_Usage(MB)\", \"Time_Per_Batch(s)\"])\n",
        "    writer.writerow(last_row)\n"
      ],
      "metadata": {
        "id": "9z6TQSOpyDak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multi-Core PyTorch Implementation"
      ],
      "metadata": {
        "id": "h-HQWOCryTNU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import time\n",
        "import psutil\n",
        "import csv\n",
        "import os\n",
        "from torchvision.models import resnet18\n",
        "\n",
        "# Use physical CPU cores\n",
        "torch.set_num_threads(psutil.cpu_count(logical=False))\n",
        "torch.set_num_interop_threads(psutil.cpu_count(logical=False))\n",
        "print(f\"[CPU mode] PyTorch using {torch.get_num_threads()} threads\")\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "# Transform\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# Datasets\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=1, shuffle=True)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=1, shuffle=False)\n",
        "\n",
        "# Model\n",
        "model = resnet18(num_classes=10)\n",
        "model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "model.maxpool = nn.Identity()\n",
        "model = model.to(device)\n",
        "\n",
        "# Loss and Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "# Accuracy Function\n",
        "def calculate_accuracy(loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loader:\n",
        "            outputs = model(images.to(device))\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct += (predicted == labels.to(device)).sum().item()\n",
        "            total += labels.size(0)\n",
        "    model.train()\n",
        "    return 100 * correct / total\n",
        "\n",
        "# CSV File\n",
        "csv_file = 'resnet18_cpu_log.csv'\n",
        "with open(csv_file, mode='w', newline='') as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow([\"Epoch\", \"Batch_Index\", \"Loss\", \"Training_Accuracy(%)\", \"Testing_Accuracy(%)\",\n",
        "                     \"CPU_Usage(%)\", \"RAM_Usage(MB)\", \"Time_Per_Batch(s)\", \"Throughput(img/sec)\"])\n",
        "\n",
        "# Training\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    total_train = 0\n",
        "    correct_train = 0\n",
        "    for batch_idx, (images, labels) in enumerate(trainloader):\n",
        "        start_time = time.perf_counter()\n",
        "\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += (predicted == labels).sum().item()\n",
        "        train_acc = 100 * correct_train / total_train\n",
        "\n",
        "        end_time = time.perf_counter()\n",
        "        elapsed = end_time - start_time\n",
        "        throughput = 1 / elapsed  # batch size = 1 → images/sec\n",
        "        global_idx = batch_idx + 1\n",
        "\n",
        "        should_log = (batch_idx == 0) or (global_idx % 10000 == 0) or (global_idx == len(trainloader))\n",
        "\n",
        "        if should_log:\n",
        "            test_acc = calculate_accuracy(testloader)\n",
        "            cpu = psutil.cpu_percent(interval=None)\n",
        "            ram = psutil.Process(os.getpid()).memory_info().rss / (1024 ** 2)\n",
        "\n",
        "            # Write to CSV\n",
        "            with open(csv_file, mode='a', newline='') as f:\n",
        "                writer = csv.writer(f)\n",
        "                writer.writerow([\n",
        "                    epoch + 1, global_idx, f\"{loss.item():.4f}\",\n",
        "                    f\"{train_acc:.2f}\", f\"{test_acc:.2f}\",\n",
        "                    f\"{cpu:.2f}\", f\"{ram:.2f}\", f\"{elapsed:.6f}\", f\"{throughput:.2f}\"\n",
        "                ])\n",
        "\n",
        "            print(f\"[Epoch {epoch+1}] Batch {global_idx}: Loss={loss.item():.4f}, \"\n",
        "                  f\"Train Acc={train_acc:.2f}%, Test Acc={test_acc:.2f}%, \"\n",
        "                  f\"CPU={cpu:.2f}%, RAM={ram:.2f}MB, Time={elapsed:.6f}s, \"\n",
        "                  f\"Throughput={throughput:.2f} img/sec\")\n",
        "\n",
        "print(f\"Training complete. Log saved to: {csv_file}\")\n"
      ],
      "metadata": {
        "id": "dpCdyuvryW6q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}