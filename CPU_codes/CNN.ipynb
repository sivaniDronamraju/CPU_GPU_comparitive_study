{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Single-Core Scratch Implementation"
      ],
      "metadata": {
        "id": "mruwHd0fefBD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import psutil\n",
        "import os\n",
        "import csv\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "process = psutil.Process(os.getpid())\n",
        "output_csv = \"scratch_cnn_singlecore.csv\"\n",
        "\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "cifar10 = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "full_data = np.stack([np.array(img[0].numpy(), dtype=np.float32) for img in cifar10])\n",
        "\n",
        "# CNN Weights and Biases\n",
        "weights1 = np.random.rand(8, 3, 3, 3).astype(np.float32)\n",
        "bias1 = np.random.rand(8).astype(np.float32)\n",
        "weights2 = np.random.rand(16, 8, 3, 3).astype(np.float32)\n",
        "bias2 = np.random.rand(16).astype(np.float32)\n",
        "weights3 = np.random.rand(32, 16, 3, 3).astype(np.float32)\n",
        "bias3 = np.random.rand(32).astype(np.float32)\n",
        "\n",
        "# CNN Operations\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def conv2d(x, w, b):\n",
        "    out_ch, in_ch, k, _ = w.shape\n",
        "    h, w_ = x.shape[1], x.shape[2]\n",
        "    out = np.zeros((out_ch, h - k + 1, w_ - k + 1), dtype=np.float32)\n",
        "    for oc in range(out_ch):\n",
        "        for ic in range(in_ch):\n",
        "            for i in range(h - k + 1):\n",
        "                for j in range(w_ - k + 1):\n",
        "                    out[oc, i, j] += np.sum(x[ic, i:i+k, j:j+k] * w[oc, ic])\n",
        "        out[oc] += b[oc]\n",
        "    return out\n",
        "\n",
        "def max_pool2d(x, size=2, stride=2):\n",
        "    c, h, w = x.shape\n",
        "    out_h = h // stride\n",
        "    out_w = w // stride\n",
        "    pooled = np.zeros((c, out_h, out_w), dtype=np.float32)\n",
        "    for ch in range(c):\n",
        "        for i in range(out_h):\n",
        "            for j in range(out_w):\n",
        "                pooled[ch, i, j] = np.max(x[ch, i*stride:i*stride+size, j*stride:j*stride+size])\n",
        "    return pooled\n",
        "\n",
        "def forward_pass(x, w1, b1, w2, b2, w3, b3):\n",
        "    x = conv2d(x, w1, b1)\n",
        "    x = relu(x)\n",
        "    x = max_pool2d(x)\n",
        "    x = conv2d(x, w2, b2)\n",
        "    x = relu(x)\n",
        "    x = max_pool2d(x)\n",
        "    x = conv2d(x, w3, b3)\n",
        "    x = relu(x)\n",
        "    x = max_pool2d(x)\n",
        "    return x\n",
        "\n",
        "# Logging CSV\n",
        "with open(output_csv, mode='w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"Image_Index\", \"CPU_Usage(%)\", \"RAM_Usage(MB)\", \"Time_Per_Image(s)\"])\n",
        "\n",
        "    print(\"\\n[Scratch CNN - Single Core] Starting forward pass on 50,000 images...\")\n",
        "    total_start = time.time()\n",
        "\n",
        "    for i, image in enumerate(full_data):\n",
        "        start = time.time()\n",
        "        output = forward_pass(image, weights1, bias1, weights2, bias2, weights3, bias3)\n",
        "        end = time.time()\n",
        "\n",
        "        time_per_image = end - start\n",
        "\n",
        "\n",
        "        if i % 500 == 0 or i == 0:\n",
        "            cpu_percent = process.cpu_percent(interval=0.1)\n",
        "            ram_usage = process.memory_info().rss / (1024 ** 2)\n",
        "            print(f\"Image {i}: CPU={cpu_percent:.2f}%, RAM={ram_usage:.2f} MB, Time={time_per_image:.4f}s\")\n",
        "        else:\n",
        "            cpu_percent = ''\n",
        "            ram_usage = ''\n",
        "\n",
        "        writer.writerow([i, f\"{cpu_percent}\", f\"{ram_usage}\", f\"{time_per_image:.6f}\"])\n",
        "\n",
        "    total_end = time.time()\n",
        "    print(f\"\\nTotal time for 50,000 images: {total_end - total_start:.2f} seconds\")\n",
        "    print(f\"Last output shape: {output.shape}\")\n",
        "\n",
        "print(f\"Results saved to: {output_csv}\")\n"
      ],
      "metadata": {
        "id": "Xxp9Qcr7eRzV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multi-Core Scratch Implementation"
      ],
      "metadata": {
        "id": "oKzzipyDexqv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "from numba import njit, prange, set_num_threads\n",
        "import psutil, time, csv, os\n",
        "\n",
        "# Configure core counts for multi-core runs\n",
        "core_counts = [1, 2, 4, 6, 8, 10]\n",
        "\n",
        "# Set up CIFAR-10 dataset\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "cifar10 = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "images = np.stack([np.array(img[0].numpy(), dtype=np.float32) for img in cifar10])  # Shape: [50000, 3, 32, 32]\n",
        "\n",
        "# Initialize random weights and biases\n",
        "weights1 = np.random.rand(8, 3, 3, 3).astype(np.float32)\n",
        "bias1 = np.random.rand(8).astype(np.float32)\n",
        "weights2 = np.random.rand(16, 8, 3, 3).astype(np.float32)\n",
        "bias2 = np.random.rand(16).astype(np.float32)\n",
        "weights3 = np.random.rand(32, 16, 3, 3).astype(np.float32)\n",
        "bias3 = np.random.rand(32).astype(np.float32)\n",
        "\n",
        "# Define CNN forward pass from scratch with Numba JIT\n",
        "@njit(parallel=True)\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "@njit(parallel=True)\n",
        "def conv2d(x, w, b):\n",
        "    out_ch, in_ch, k, _ = w.shape\n",
        "    h, w_ = x.shape[1], x.shape[2]\n",
        "    out = np.zeros((out_ch, h - k + 1, w_ - k + 1), dtype=np.float32)\n",
        "    for oc in prange(out_ch):\n",
        "        for ic in range(in_ch):\n",
        "            for i in range(h - k + 1):\n",
        "                for j in range(w_ - k + 1):\n",
        "                    out[oc, i, j] += np.sum(x[ic, i:i + k, j:j + k] * w[oc, ic])\n",
        "        out[oc] += b[oc]\n",
        "    return out\n",
        "\n",
        "@njit(parallel=True)\n",
        "def max_pool2d(x, size=2, stride=2):\n",
        "    c, h, w = x.shape\n",
        "    out_h = h // stride\n",
        "    out_w = w // stride\n",
        "    pooled = np.zeros((c, out_h, out_w), dtype=np.float32)\n",
        "    for ch in prange(c):\n",
        "        for i in range(out_h):\n",
        "            for j in range(out_w):\n",
        "                pooled[ch, i, j] = np.max(x[ch, i*stride:i*stride+size, j*stride:j*stride+size])\n",
        "    return pooled\n",
        "\n",
        "@njit(parallel=True)\n",
        "def forward_pass(x, w1, b1, w2, b2, w3, b3):\n",
        "    x = conv2d(x, w1, b1)\n",
        "    x = relu(x)\n",
        "    x = max_pool2d(x)\n",
        "    x = conv2d(x, w2, b2)\n",
        "    x = relu(x)\n",
        "    x = max_pool2d(x)\n",
        "    x = conv2d(x, w3, b3)\n",
        "    x = relu(x)\n",
        "    x = max_pool2d(x)\n",
        "    return x\n",
        "\n",
        "# Prepare CSV\n",
        "csv_file = \"cnn_fromscratch_multicore_metrics.csv\"\n",
        "process = psutil.Process(os.getpid())\n",
        "\n",
        "with open(csv_file, mode='w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"Core_Count\", \"Image_Index\", \"CPU_Usage(%)\", \"RAM_Usage(MB)\", \"Time_Per_Image(s)\"])\n",
        "\n",
        "    for core_count in core_counts:\n",
        "        set_num_threads(core_count)\n",
        "        print(f\"\\n======== Running on {core_count} Core(s) ========\")\n",
        "        total_start = time.time()\n",
        "\n",
        "        for i in range(len(images)):\n",
        "            img = images[i]\n",
        "            start = time.time()\n",
        "            output = forward_pass(img, weights1, bias1, weights2, bias2, weights3, bias3)\n",
        "            end = time.time()\n",
        "\n",
        "            cpu_percent = process.cpu_percent(interval=None)\n",
        "            ram_usage = process.memory_info().rss / (1024 ** 2)\n",
        "            time_per_image = end - start\n",
        "\n",
        "            if i % 1000 == 0:\n",
        "                print(f\"[Cores={core_count}] Image {i}: CPU={cpu_percent:.2f}%, RAM={ram_usage:.2f} MB, Time={time_per_image:.4f}s\")\n",
        "\n",
        "            writer.writerow([core_count, i, f\"{cpu_percent:.2f}\", f\"{ram_usage:.2f}\", f\"{time_per_image:.6f}\"])\n",
        "\n",
        "        total_end = time.time()\n",
        "        print(f\"[Cores={core_count}] Total Time: {total_end - total_start:.2f} seconds\")\n",
        "        print(f\"[Cores={core_count}] Output shape: {output.shape}\")"
      ],
      "metadata": {
        "id": "yxhlTvPme2QO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Single-Core PyTorch Implementation"
      ],
      "metadata": {
        "id": "Vfe0ESkZx-Na"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import time\n",
        "import psutil\n",
        "import csv\n",
        "import os\n",
        "\n",
        "# PyTorch 1 thread\n",
        "torch.set_num_threads(1)\n",
        "torch.set_num_interop_threads(1)\n",
        "print(f\"[Single-core mode] PyTorch using {torch.get_num_threads()} thread(s)\")\n",
        "\n",
        "# Define transform\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=1, shuffle=True)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=1, shuffle=False)\n",
        "\n",
        "# CNN Model\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "        )\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(32 * 8 * 8, 100),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(100, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Initialize model\n",
        "model = SimpleCNN()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "# CSV for logging\n",
        "csv_file = 'pytorch_cnn_singlecore_with_accuracy.csv'\n",
        "with open(csv_file, mode='w', newline='') as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow([\"Epoch\", \"Batch_Index\", \"Loss\", \"Training_Accuracy(%)\", \"Testing_Accuracy(%)\", \"CPU_Usage(%)\", \"RAM_Usage(MB)\", \"Time_Per_Batch(s)\"])\n",
        "\n",
        "# Calculate accuracy\n",
        "def calculate_accuracy(loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loader:\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    return 100 * correct / total\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "model.train()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "    for batch_idx, (images, labels) in enumerate(trainloader):\n",
        "        start = time.perf_counter()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Calculate training accuracy\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "        end = time.perf_counter()\n",
        "\n",
        "        # Metrics\n",
        "        cpu = psutil.cpu_percent(interval=None)\n",
        "        ram = psutil.Process(os.getpid()).memory_info().rss / (1024 * 1024)\n",
        "        elapsed = end - start\n",
        "        train_accuracy = 100 * correct_train / total_train\n",
        "\n",
        "        # Calculate testing accuracy at the end of each epoch\n",
        "        test_accuracy = calculate_accuracy(testloader)\n",
        "\n",
        "        with open(csv_file, mode='a', newline='') as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow([epoch + 1, batch_idx, f\"{loss.item():.4f}\", f\"{train_accuracy:.2f}\", f\"{test_accuracy:.2f}\", f\"{cpu:.2f}\", f\"{ram:.2f}\", f\"{elapsed:.6f}\"])\n",
        "\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(f\"[Epoch {epoch+1}] Batch {batch_idx}: Loss={loss.item():.4f}, Train Accuracy={train_accuracy:.2f}%, Test Accuracy={test_accuracy:.2f}%, CPU={cpu:.2f}%, RAM={ram:.2f}MB, Time={elapsed:.6f}s\")\n",
        "\n",
        "print(f\"CNN training (single-core) complete. Logged to: {csv_file}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "9z6TQSOpyDak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multi-Core PyTorch Implementation"
      ],
      "metadata": {
        "id": "h-HQWOCryTNU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import time\n",
        "import psutil\n",
        "import csv\n",
        "import os\n",
        "\n",
        "# Use all physical CPU cores\n",
        "torch.set_num_threads(psutil.cpu_count(logical=False))\n",
        "torch.set_num_interop_threads(psutil.cpu_count(logical=False))\n",
        "print(f\"[Multi-core mode] PyTorch using {torch.get_num_threads()} thread(s)\")\n",
        "\n",
        "# Transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# CIFAR-10 datasets\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n",
        "\n",
        "# CNN model\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "        )\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(32 * 8 * 8, 100),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(100, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Init\n",
        "model = SimpleCNN()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "# CSV log file\n",
        "csv_file = 'pytorch_cnn_multicore_optimized.csv'\n",
        "with open(csv_file, mode='w', newline='') as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow([\"Epoch\", \"Batch_Index\", \"Loss\", \"Training_Accuracy(%)\", \"Testing_Accuracy(%)\", \"CPU_Usage(%)\", \"RAM_Usage(MB)\", \"Time_Per_Batch(s)\"])\n",
        "\n",
        "# Accuracy function\n",
        "def calculate_accuracy(loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loader:\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    return 100 * correct / total\n",
        "\n",
        "# Training\n",
        "num_epochs = 10\n",
        "model.train()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "    for batch_idx, (images, labels) in enumerate(trainloader):\n",
        "        start = time.perf_counter()\n",
        "\n",
        "        # Forward/backward/update\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Training accuracy\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += (predicted == labels).sum().item()\n",
        "        train_accuracy = 100 * correct_train / total_train\n",
        "\n",
        "        end = time.perf_counter()\n",
        "        elapsed = end - start\n",
        "\n",
        "        # Logging condition\n",
        "        should_log = (batch_idx == 0) or (batch_idx % 10000 == 0) or (batch_idx == len(trainloader) - 1)\n",
        "        if should_log:\n",
        "            test_accuracy = calculate_accuracy(testloader)\n",
        "            cpu = psutil.cpu_percent(interval=None)\n",
        "            ram = psutil.Process(os.getpid()).memory_info().rss / (1024 * 1024)\n",
        "\n",
        "            with open(csv_file, mode='a', newline='') as f:\n",
        "                writer = csv.writer(f)\n",
        "                writer.writerow([\n",
        "                    epoch + 1, batch_idx, f\"{loss.item():.4f}\",\n",
        "                    f\"{train_accuracy:.2f}\", f\"{test_accuracy:.2f}\",\n",
        "                    f\"{cpu:.2f}\", f\"{ram:.2f}\", f\"{elapsed:.6f}\"\n",
        "                ])\n",
        "\n",
        "            print(f\"[Epoch {epoch+1}] Batch {batch_idx}: Loss={loss.item():.4f}, Train Acc={train_accuracy:.2f}%, Test Acc={test_accuracy:.2f}%, CPU={cpu:.2f}%, RAM={ram:.2f}MB, Time={elapsed:.6f}s\")\n",
        "\n",
        "print(f\"CNN training (multi-core) complete. Logged to: {csv_file}\")"
      ],
      "metadata": {
        "id": "dpCdyuvryW6q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}